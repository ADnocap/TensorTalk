{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorTalk Evaluation\n",
    "\n",
    "This notebook demonstrates how to evaluate TensorTalk using the metrics from the paper:\n",
    "- **UTMOS**: UTokyo-SaruLab Mean Opinion Score (naturalness)\n",
    "- **WER**: Word Error Rate (intelligibility)\n",
    "- **PER**: Phoneme Error Rate (intelligibility)\n",
    "- **SECS**: Speaker Encoder Cosine Similarity (speaker similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, import necessary libraries and initialize the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src import TensorTalkPipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = TensorTalkPipeline(k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Sentences\n",
    "\n",
    "Load the test sentences from the FLoRes+ dataset (or your own test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test sentences\n",
    "sentences_df = pd.read_csv('../data/sampled_sentences.csv', header=None, names=['text'])\n",
    "test_sentences = sentences_df['text'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(test_sentences)} test sentences\")\n",
    "print(f\"\\nFirst 3 sentences:\")\n",
    "for i, text in enumerate(test_sentences[:3]):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Speech\n",
    "\n",
    "Generate synthetic speech for all test sentences using a target speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure target speaker\n",
    "target_audio_paths = [\n",
    "    \"../examples/target_speaker/sample1.wav\",\n",
    "    \"../examples/target_speaker/sample2.wav\",\n",
    "]\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"../examples/generated/evaluation\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate speech for each sentence\n",
    "print(\"Generating synthetic speech...\")\n",
    "generated_files = []\n",
    "\n",
    "for i, text in enumerate(tqdm(test_sentences[:10]))  # Test on first 10 sentences:\n",
    "    try:\n",
    "        # Synthesize\n",
    "        audio = pipeline.synthesize(\n",
    "            text=text,\n",
    "            target_audio_paths=target_audio_paths,\n",
    "            alpha=1.0\n",
    "        )\n",
    "        \n",
    "        # Save\n",
    "        output_path = output_dir / f\"generated_{i:03d}.wav\"\n",
    "        pipeline.save_audio(audio, str(output_path))\n",
    "        generated_files.append(str(output_path))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError generating sentence {i}: {e}\")\n",
    "        generated_files.append(None)\n",
    "\n",
    "print(f\"\\nGenerated {len([f for f in generated_files if f])} audio files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate WER (Word Error Rate)\n",
    "\n",
    "Calculate WER using automatic speech recognition (Whisper).\n",
    "\n",
    "**Note**: You'll need to install additional packages:\n",
    "```bash\n",
    "pip install openai-whisper jiwer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder - install whisper and jiwer to use\n",
    "# Uncomment after installing:\n",
    "\n",
    "# import whisper\n",
    "# import jiwer\n",
    "\n",
    "# # Load Whisper model\n",
    "# model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "# wer_scores = []\n",
    "# for i, (ref_text, audio_path) in enumerate(zip(test_sentences[:10], generated_files)):\n",
    "#     if audio_path is None:\n",
    "#         continue\n",
    "#     \n",
    "#     # Transcribe\n",
    "#     result = model.transcribe(audio_path)\n",
    "#     hyp_text = result[\"text\"]\n",
    "#     \n",
    "#     # Calculate WER\n",
    "#     wer = jiwer.wer(ref_text, hyp_text)\n",
    "#     wer_scores.append(wer)\n",
    "#     \n",
    "#     print(f\"Sentence {i}:\")\n",
    "#     print(f\"  Reference: {ref_text}\")\n",
    "#     print(f\"  Hypothesis: {hyp_text}\")\n",
    "#     print(f\"  WER: {wer:.3f}\\n\")\n",
    "\n",
    "# print(f\"Average WER: {np.mean(wer_scores):.3f} ± {np.std(wer_scores):.3f}\")\n",
    "\n",
    "print(\"WER evaluation requires whisper and jiwer packages.\")\n",
    "print(\"Install with: pip install openai-whisper jiwer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate SECS (Speaker Encoder Cosine Similarity)\n",
    "\n",
    "Measure speaker similarity between generated and target speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires the ECAPA2 speaker encoder\n",
    "# Placeholder implementation:\n",
    "\n",
    "# from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "# # Load speaker encoder\n",
    "# spk_encoder = EncoderClassifier.from_hparams(\n",
    "#     source=\"speechbrain/spkrec-ecapa-voxceleb\"\n",
    "# )\n",
    "\n",
    "# secs_scores = []\n",
    "# for audio_path in generated_files:\n",
    "#     if audio_path is None:\n",
    "#         continue\n",
    "#     \n",
    "#     # Extract embeddings\n",
    "#     gen_emb = spk_encoder.encode_batch(audio_path)\n",
    "#     target_emb = spk_encoder.encode_batch(target_audio_paths[0])\n",
    "#     \n",
    "#     # Calculate cosine similarity\n",
    "#     similarity = F.cosine_similarity(gen_emb, target_emb, dim=-1)\n",
    "#     secs_scores.append(similarity.item())\n",
    "\n",
    "# print(f\"Average SECS: {np.mean(secs_scores):.3f} ± {np.std(secs_scores):.3f}\")\n",
    "\n",
    "print(\"SECS evaluation requires SpeechBrain package.\")\n",
    "print(\"Install with: pip install speechbrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Compare results with the paper's reported metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['UTMOS ↑', 'WER ↓', 'PER ↓', 'SECS ↑'],\n",
    "    'Paper (LJSpeech)': [4.27, 0.02, 0.01, 0.65],\n",
    "    'Your Results': ['N/A', 'N/A', 'N/A', 'N/A'],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote: Install evaluation packages to calculate metrics.\")\n",
    "print(\"See paper for baseline comparisons: https://drive.google.com/file/d/1j9t6o2sKrWnu83dZkSFWDphGyeI9_NYr/view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Analysis\n",
    "\n",
    "Listen to some examples to assess quality subjectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Play first 3 generated samples\n",
    "for i, audio_path in enumerate(generated_files[:3]):\n",
    "    if audio_path is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nSample {i+1}: {test_sentences[i][:50]}...\")\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    display(Audio(waveform.numpy(), rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a framework for evaluating TensorTalk's performance.\n",
    "\n",
    "### Key Metrics:\n",
    "- **UTMOS**: Measures naturalness (how human-like the speech sounds)\n",
    "- **WER/PER**: Measure intelligibility (how accurate the words/phonemes are)\n",
    "- **SECS**: Measures speaker similarity (how close to target voice)\n",
    "\n",
    "### Results from Paper:\n",
    "- Achieved UTMOS of **4.27** on LJSpeech\n",
    "- Near-perfect WER/PER due to gTTS (trade-off: less natural prosody)\n",
    "- Competitive SECS showing good voice transfer\n",
    "\n",
    "For full results and analysis, see the [paper](https://drive.google.com/file/d/1j9t6o2sKrWnu83dZkSFWDphGyeI9_NYr/view)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
