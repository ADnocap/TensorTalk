{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorTalk: Zero-Shot Voice Cloning Demo\n",
    "\n",
    "This notebook demonstrates how to use TensorTalk for zero-shot voice cloning.\n",
    "\n",
    "**Paper**: [TensorTalk: A Voice Cloning Framework for Text-to-Speech Synthesis Using WavLM Features and Rhythm Modeling](https://drive.google.com/file/d/1j9t6o2sKrWnu83dZkSFWDphGyeI9_NYr/view)\n",
    "\n",
    "**GitHub**: [github.com/robrich07/TensorTalk](https://github.com/robrich07/TensorTalk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's import the necessary libraries and initialize the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from src import TensorTalkPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Pipeline\n",
    "\n",
    "Initialize the TensorTalk pipeline with default settings:\n",
    "- **k=4**: Number of nearest neighbors for voice conversion\n",
    "- **device**: Automatically selects CUDA if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = TensorTalkPipeline(k=4)\n",
    "\n",
    "print(f\"Using device: {pipeline.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Target Speaker Audio\n",
    "\n",
    "For zero-shot voice cloning, you need reference audio from the target speaker.\n",
    "The more reference audio you provide, the better the quality.\n",
    "\n",
    "**Recommendations:**\n",
    "- Minimum: 3-5 seconds of clean speech\n",
    "- Optimal: 10-30 seconds from multiple utterances\n",
    "- Quality: Clear audio with minimal background noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to target speaker audio file(s)\n",
    "# You can provide a single file or a list of files\n",
    "target_audio_paths = [\n",
    "    \"../examples/target_speaker/sample1.wav\",\n",
    "    \"../examples/target_speaker/sample2.wav\",\n",
    "    \"../examples/target_speaker/sample3.wav\",\n",
    "]\n",
    "\n",
    "# Or use a single file\n",
    "# target_audio_paths = \"../examples/target_speaker/sample.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text-to-Speech Synthesis\n",
    "\n",
    "Now let's synthesize speech in the target speaker's voice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to synthesize\n",
    "text = \"Hello! This is a demonstration of zero-shot voice cloning using TensorTalk.\"\n",
    "\n",
    "# Synthesize speech\n",
    "# alpha controls voice conversion strength:\n",
    "#   - alpha=1.0: Full target voice (default)\n",
    "#   - alpha=0.5: Mix of source and target\n",
    "#   - alpha=0.0: Original source voice (gTTS)\n",
    "\n",
    "audio = pipeline.synthesize(\n",
    "    text=text,\n",
    "    target_audio_paths=target_audio_paths,\n",
    "    alpha=1.0,  # Full voice conversion\n",
    "    lang='en'   # Language code for gTTS\n",
    ")\n",
    "\n",
    "print(\"Synthesis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Listen to the Result\n",
    "\n",
    "Play the synthesized audio directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display audio player\n",
    "display(Audio(audio.cpu().numpy(), rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize the Waveform\n",
    "\n",
    "Let's visualize the generated audio waveform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot waveform\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(audio.cpu().numpy())\n",
    "plt.title(\"Generated Speech Waveform\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Audio\n",
    "\n",
    "Save the synthesized audio to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save audio to file\n",
    "output_path = \"../examples/generated/output.wav\"\n",
    "pipeline.save_audio(audio, output_path, sample_rate=16000)\n",
    "\n",
    "print(f\"Audio saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment with Voice Blending\n",
    "\n",
    "Try different alpha values to blend between source and target voices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha values\n",
    "alphas = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "text = \"Testing voice blending with different alpha values.\"\n",
    "\n",
    "results = []\n",
    "for alpha in alphas:\n",
    "    print(f\"\\nGenerating with alpha={alpha}...\")\n",
    "    audio = pipeline.synthesize(\n",
    "        text=text,\n",
    "        target_audio_paths=target_audio_paths,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    results.append((alpha, audio))\n",
    "    \n",
    "    # Save each result\n",
    "    output_path = f\"../examples/generated/alpha_{alpha:.2f}.wav\"\n",
    "    pipeline.save_audio(audio, output_path)\n",
    "\n",
    "print(\"\\nAll variations generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Voice Blending Results\n",
    "\n",
    "Listen to each version to hear the gradual transition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha, audio in results:\n",
    "    print(f\"\\nAlpha = {alpha} ({'Source' if alpha == 0 else 'Target' if alpha == 1 else 'Mixed'})\")\n",
    "    display(Audio(audio.cpu().numpy(), rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Direct Component Usage\n",
    "\n",
    "For more control, you can use the individual components directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import SSLEncoder, KNNMatcher\n",
    "\n",
    "# Initialize components separately\n",
    "ssl_encoder = SSLEncoder()\n",
    "knn_matcher = KNNMatcher(k=4)\n",
    "\n",
    "# Load and extract features from an audio file\n",
    "audio_path = \"../examples/target_speaker/sample1.wav\"\n",
    "features = ssl_encoder.extract_from_file(audio_path)\n",
    "\n",
    "print(f\"Extracted features shape: {features.shape}\")\n",
    "print(f\"Feature dimension: {features.shape[-1]}\")\n",
    "print(f\"Number of frames: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing\n",
    "\n",
    "Process multiple sentences efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sentences to synthesize\n",
    "sentences = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Here comes the second one.\",\n",
    "    \"And finally, the third sentence.\",\n",
    "]\n",
    "\n",
    "# Process each sentence\n",
    "for i, text in enumerate(sentences):\n",
    "    print(f\"\\nProcessing sentence {i+1}/{len(sentences)}...\")\n",
    "    \n",
    "    audio = pipeline.synthesize(\n",
    "        text=text,\n",
    "        target_audio_paths=target_audio_paths,\n",
    "        alpha=1.0\n",
    "    )\n",
    "    \n",
    "    output_path = f\"../examples/generated/sentence_{i+1}.wav\"\n",
    "    pipeline.save_audio(audio, output_path)\n",
    "\n",
    "print(\"\\nBatch processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "1. Initialize the TensorTalk pipeline\n",
    "2. Synthesize speech in a target speaker's voice\n",
    "3. Control voice conversion with the alpha parameter\n",
    "4. Save and visualize results\n",
    "5. Process multiple sentences efficiently\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with your own voice samples\n",
    "- Experiment with different k values (number of neighbors)\n",
    "- Integrate rhythm modeling for more natural speech\n",
    "- Check out the paper for technical details\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Paper**: https://drive.google.com/file/d/1j9t6o2sKrWnu83dZkSFWDphGyeI9_NYr/view\n",
    "- **GitHub**: https://github.com/robrich07/TensorTalk\n",
    "- **Issues**: Report bugs or request features on GitHub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
